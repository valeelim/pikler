Task parallelism     task parallelism ( also known as thread level parallelism , function parallelism and control parallelism ) is a form of parallel computing for multiple processors using a technique for distributing execution of processes and threads across different parallel processor nodes . it contrasts to data parallelism as another form of parallelism . in a multiprocessor system , task parallelism is achieved when each processor executes a different thread ( or process ) on the same or different data . the threads may execute the same or different code . different execution threads communicate with one another usually to pass data as they work . as a simple example , if we are running code on a 2-processor system ( cpus `` a '' & `` b '' ) in a parallel computing environment and we want to do tasks `` a '' and `` b '' , it is possible to tell cpu `` a '' to do task `` a '' and cpu `` b '' to do task 'b '' simultaneously ( at the same time ) , in order to reduce the runtime of the execution . task parallelism is used by multi-user and multitasking operating systems , and applications depending on processes and threads , unlike data processing applications ( see data parallelism ) . most real programs use a combination of task parallelism and data parallelism .