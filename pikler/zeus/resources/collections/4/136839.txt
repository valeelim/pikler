Information theory     information theory is a branch of applied mathematics and electrical engineering . information theory measures the amount of information in data that could have more than one value . in its most common use , information theory finds physical and mathematical limits on the amounts of data in data compression and data communication . data compression and data communication are statistical , because they guess unknown values . the amount of information in data measures how easily it is guessed by a person who does not know its value . a key measure in information theory is `` entropy '' . entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process . for example , identifying the outcome of a fair coin flip ( with two equally likely outcomes ) provides less information ( lower entropy ) than specifying the outcome from a roll of a dice ( with six equally likely outcomes ) . some other important measures in information theory are mutual information , channel capacity , error exponents , and relative entropy .