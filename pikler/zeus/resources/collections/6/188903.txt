Big O notation     big o notation is a way of saying how much time it takes for a mathematical algorithm to run or how much memory it uses . the big o notation is often used in identifying how complex a problem is ( also known as the problem 's complexity class ) . it is an expression that is meant to give a person a feel for the worst case time and/or space requirements of a program without them having to actually code and run the program on a real computer . this is also advantageous since different computers may have different hardware specifications and may produce the answer in different amounts of time should any program be run on them . the big o notation , on the other hand , remains the same even if you 're running a program on your desktop , a server or a supercomputer . let us see how big o is used in practice . consider for example that we 're trying to find the maximum number from a given set of numbers . to find it , we have to look at each and every number in the set . if we were given twice the numbers , we would take twice as long to find the maximum number . thus , finding the maximum in a set is formula_1 - the time or number of steps required to find an answer increases or decreases directly with increase or decrease in the size of the input . on the other hand , if an algorithm were said to be formula_2 , then the time taken to produce the output would take a time that would square in magnitude with change in size of the input - for e.g . if someone gave an input that was twice as large as an initial input , the time taken would be formula_3 times as large .