Algorithmic information theory     algorithmic information theory is a field of theoretical computer science . it is concerned with how information and computation are related . most information can be represented as a string ( or a sequence of characters ) . algorithmic information theory studies the complexity of information represented that way ( in other words , how difficult it is to get that information , or how long it takes ) . unlike regular information theory , it uses kolmogorov complexity to describe complexity , and not the measure of complexity developed by claude shannon and warren weaver . kolmogorov complexity was developed independently by andrey kolmogorov and gregory chaitin . according to claude shannon the following two binary strings have the same content in information ( this is only valid for the first-order entropy ) : the first was generated with a random number generator , for example by throwing a coin . the second is easier to describe ( eight times `` 1 '' , then eight times `` 0 '' ) . for this reason , the first sequence has more algorithmic information , because it is harder to shorten ( `` compress '' ) the description on how to generate it . shortening the description may not be possible at all . the information value of a string is higher , if it is more difficult to shorten ( `` compress '' ) its description . random strings and white noise do not contain patterns that occur again . for this reason they can not be compressed , and have a higher information value .