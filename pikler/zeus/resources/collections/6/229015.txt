Markov chain     a markov chain is a model of some random process that happens over time . markov chains are called that because they follow a rule called the markov property . the markov property says that whatever happens next in a process only depends on how it is right now ( the state ) . it does n't have a `` memory '' of how it was before . it is helpful to think of a markov chain as evolving through discrete steps in time , although the `` step '' does n't need to have anything to do with time . markov chains can be discrete or continuous . discrete time markov chains are split up into discrete time steps , like t = 1 , t = 2 , t = 3 , and so on . the probability that a chain will go from one state to another state depends only on the state that it 's in right now . continuous time markov chains are chains where the time spent in each state is a real number . the amount of time the chain stays in a certain state is randomly picked from an exponential distribution , which basically means there 's an average time a chain will stay in some state , plus or minus some random variation .