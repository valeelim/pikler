Robots exclusion standard     the robots exclusion standard ( also called the robots exclusion protocol or robots.txt protocol ) is a way of telling web crawlers and other web robots which parts of a web site they can see . to give robots instructions about which pages of a web site they can access , site owners put a text file called robots.txt in the main directory of their web site , e.g . http : //www.example.com/robots.txt . this text file tells robots which parts of the site they can and can not access . however , robots can ignore robots.txt files , especially malicious ( bad ) robots . if the robots.txt file does not exist , web robots assume that they can see all parts of the site . examples of robots.txt files . - google - youtube - wikipedia