Information entropy     information entropy is a concept from information theory . it tells how much information there is in an event . in general , the more uncertain or random the event is , the more information it will contain . more clearly stated , information is a decrease in uncertainty or entropy . the concept of information entropy was created by mathematician claude shannon . information and its relationship to entropy can be modeled by : r = h ( x ) - hy ( x ) '' the conditional entropy hy ( x ) will , for convenience , be called the equivocation . it measures the average ambiguity of the received signal . '' the `` average ambiguity '' or hy ( x ) meaning uncertainty or entropy . h ( x ) represents information . r is the received signal . it has applications in many areas , including lossless data compression , statistical inference , cryptography , and sometimes in other disciplines as biology , physics or machine learning . the information gain is a measure of the probability with which a certain result is expected to happen . in the context of a coin flip , with a 50-50 probability , the entropy is the highest value of 1. it does not involve information gain because it does not incline towards a specific result more than the other . if there is a 100-0 probability that a result will occur , the entropy is 0 .