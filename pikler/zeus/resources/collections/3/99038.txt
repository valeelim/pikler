Character (computing)    character ( computing ) for computers , a character is a letter , number , or punctuation mark that can be shown on a screen , or printed . there are also some unseen characters , called `` control characters '' . an example of a control character is the carriage return or line feed that tells the software to start a new line . a `` glyph '' is the shape of the character that is seen when put on screen or on paper . since computers only use numbers , they use number codes to represent characters . for example , in ascii , the number 64 represents the letter 'a ' . the computer knows , when working in ascii , to put the glyph for 'a ' on the screen when it sees the number 64. the glyph can change shape slightly depending on the font that is used , if it is `` 'bold '' or in `` italics '' , etc . but it is still stored as a 64 in the computer , that does not change . one of the early standards for storing characters was ascii . it uses 7 bits to store each character . this allows 128 characters - enough for the numbers 0-9 , the upper case letters a-z , lower case letters a-z , most punctuation marks including spaces , parentheses and braces , and some control characters . but it is not enough for letters used in other languages , such as umlauts used in german and scandinavian languages . some systems added an 8th bit to ascii , for another 128 characters , but the extra characters can vary from one system to another . and this does n't address all languages . there are thousands of chinese characters alone . to store all of these characters , unicode was developed . this uses 16 bits to store 65,536 characters .